---
layout:       post
title:        "Entropy, cross Entropy and KL Divergence"
subtitle:     "Entropy, cross Entropy and KL Divergence"
date:         2018-06-07 09:07:00
author:       "xuepro"
header-img:   "img/home_bg.jpg"
header-mask:  0.3
catalog:      true
multilingual: true
tags:
    - AI
    
---

**Information entropy** is defined as the average amount of information produced by a stochastic source of data.
(熵是随机数据的平均信息量)， 一个随机事件发生的概率越大，其信息量越少，反之，一个事件发生的概率越小，则一旦发生其信息量就很大，比如你说“一个人最终会死亡”，因为这是一个确定性的事件，即发生概率为1，因此其信息量为0.也就是说大家都知道这是必然发生的事情，所以这句话的信息为0，即没有任何有价值的信息。但如果你说“朝鲜完全销毁了核武器，成为美国的盟国”，这将是一个爆炸性新闻，也就说信息量很大，因为这是一个非常小概率事件。

由于信息量的大小和概率的大小是相反的，我们可以用一个概率p的倒数来刻画概率p事件的信息量。即$$ \frac{1}{p}$$。

另外，2个独立事件同时发生的信息量应该是各自事件发生的信息量之和。两个事件独立发生概率假如是$$p_i$$和$$p_j$$，则同时发生的概率为$$p_ip_j$$。而用倒数刻画信息量则不满足这个条件：

 $$H( \frac{1}{p_ip_j}) \neq \frac{1}{p_i}+ \frac{1}{p_j} $$
 
 由于取对数，正好可以满足这个可加性：
 
 $$log( \frac{1}{p_ip_j}) == log(\frac{1}{p_i})+ log(\frac{1}{p_j}) $$。
 
 因此，我们可以用$$log(\frac{1}{p})$$或$$log_2(\frac{1}{p})$$来刻画一个发生概率为p的事件的信息量。
 
 如果一个随机变量的可能取值为$$X = { x_1,x_2,\dots,x_n }$$，对应的概率为$$P(X=x_i) = { p_1,p_2,\dots,p_n } $$，则随机变量的**熵**定义为
 
 $$H(X)  = - \sum_i^{n}p(x_i)log(p(x_i))$$
 
 即**熵**是所有可能发生事件的**信息量的平均（期望）** 。
 
 熵也可以理解为对随机事件进行区分所需的最少编码长度。
 
 **cross Entropy** :假如一个随机事件真正的概率分布是 $$P_{data}(X=x_i) = { p_1,p_2,\dots,p_n } $$，而我们对此随机事件的预测概率分布是$$P_{model}(X=x_i) = { q_1,q_2,\dots,q_n } $$。根据我们预测的事件概率分布对随机事件编码的长度为：
 
  $$H(p,q)  = - \sum_i^{n}p(x_i)log(q(x_i))$$
  
  这个量称为**cross Entropy 交叉熵**,它刻画了2个概率分布的差异程度，当q和p正好一致，正好达到最低（小）值（最少/最佳的编码量），但如果不一样，则出现了冗余，这个冗余就称为**KL Divergence(KL散度)** 。
  
  $$D_{KL}(p \parallel q)  = H(p,q) - H(p)$$
  
 

[A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)

[A Friendly Introduction to Cross-Entropy Loss](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)
