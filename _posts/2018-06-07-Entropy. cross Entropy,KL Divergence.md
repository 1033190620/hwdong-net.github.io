---
layout:       post
title:        "Entropy, cross Entropy and KL Divergence"
subtitle:     "Entropy, cross Entropy and KL Divergence"
date:         2018-06-07 09:07:00
author:       "xuepro"
header-img:   "img/home_bg.jpg"
header-mask:  0.3
catalog:      true
multilingual: true
tags:
    - AI
    
---

**Information entropy** is defined as the average amount of information produced by a stochastic source of data.
(熵是随机信息的平均信息量)， 一个随机事件发生的概率越大，其信息量越少，反之，一个事件发生的概率越小，则一旦发生其信息量就很大，比如你说“一个人最终会死亡”，因为这是一个确定性的事件，即发生概率为1，因此其信息量为0.也就是说大家都知道这是必然发生的事情，所以这句话的信息为0，即没有任何有价值的信息。但如果你说“朝鲜完全销毁了核武器，成为美国的盟国”，这将是一个爆炸性新闻，也就说信息量很大，因为这是一个非常小概率事件。

由于信息量的大小和概率的大小是相反的，我们可以用一个概率p的倒数来刻画概率p事件的信息量。即$$H( \frac{1}{p}$$。

另外，2个独立事件同时发生的信息量应该鸽子发生的信息量之和。两个事件独立发生概率假如是$$p_i$$和$$p_j$$，则同时发生的概率为$$p_ip_j$$。而用倒数刻画信息量则不满足这个条件：

 $$H( \frac{1}{p_ip_j} \neq \frac{1}{p_i}+ \frac{1}{p_j} $$。
 
 由于取对数，正好可以满足这个可加性：
 
 $$H(log( \frac{1}{p_ip_j}) == log(\frac{1}{p_i})+ log(\frac{1}{p_j}) $$。
 
 因此，我们可以用$$log(p)$$或$$log_2(p)$$来刻画一个发生概率为p的事件的信息量。
 
 
[A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)
[A Friendly Introduction to Cross-Entropy Loss](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)
