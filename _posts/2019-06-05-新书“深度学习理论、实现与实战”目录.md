---
layout:       post
title:        新书“深度学习理论、实现与实战”目录
subtitle:     新书“深度学习理论、实现与实战”目录
date:         2019-06-05 07:06:00
author:       "xuepro"
header-img:   "img/home_bg.jpg"
header-mask:  0.3
catalog:      true
multilingual: true
tags:
    - DL
---

打算写一本新书 **深度学习理论、实现与实战** ，目录初步如下，欢迎意见和建议，比如选取什么样的案例？
注：为避免目录细节太多，很多小节的细节省略了。

 ```
第1章  基础知识       
       1.1  数学基础
          1.1.1 线性代数: 向量、矩阵、范数、特征值和特征向量
          1.1.2 函数、导数、梯度、Heisen矩阵
          1.1.3 优化
          1.1.4  概率和统计
      1.2  Python快速入门 
          1.2.1 Python介绍及环境的安装
          1.2.2 基本计算
          1.2.3 数据结构
          1.2.4 函数
          1.2.5 类和对象          
       1.3  Matplot画图 
       1.4 Numpy快速入门       
       
第2章  回归和分类：线性回归、逻辑回归、softmax回归
     2.1 什么是机器学习?
     2.2 线性回归
       2.2.1 餐车利润问题
       2.2.2 什么是线性回归？
       2.2.3 模型训练：求解假设函数       
     2.3 正规方程
     2 .4 梯度下降法
        2.4.1 梯度下降法
        2.4.2 梯度下降法的python实现
        2.4.3 调试学习率
        2.4.4 梯度验证
        2.4.5 梯度下降法的 numpy实现
        2.4.6 随机梯度下降法和批梯度下降法
        2.4.7 数据的规范化
     2.5 欠拟合和过拟合、学习曲线
     2.6 实战Kaggle比赛: 房屋价格预测
     2.7 逻辑回归
         2.7.1 二分类和逻辑回归
         2.7.2 逻辑回归的numpy实现
         2.7.3 Scikit-Learn 的逻辑回归模型
         2.7.4 实战：鸢尾花的分类
     2.8 softmax回归
         2.8.1 多分类和softmax回归
         2.8.2 softmax回归的numpy实现
         2.8.3 实战：MNIST手写数字识别 
        
第3章  神经网络和深度学习
     3.1 神经网络
        3.1.1 神经元
        3.1.2 激活函数
        3.1.3 神经网络       
        3.1.4 损失函数
     3.2 神经网络的训练
         3.2.1 代价函数
         3.2.2 求导：反向传播算法
               链式求导法则
               激活函数的导数
               用链式求导法则求代价函数的梯度
               反向传播算法
         3.2.3 梯度下降算法的numpy实现
     3.3 实战：3层神经网络识别MNIST手写数字
           1.3.1  MNIST手写数字集
           1.3.2  神经网络的训练
           1.3.3  神经网络的预测
     3.4 深度学习
        3.4.1  深层神经网络
        3.4.2  前向传播、反向传播和计算图
        3.4.3  实战：MNIST手写数字识别的深度学习
        3.4.4  多层神经网络的训练技巧(参数和超参数)    
     
第4章  深度学习平台PyTorch和TensorFlow
     4.1  Pytorch基础知识
          4.1.1 安装
          4.1.2 张量（Tensors）
          4.1.3 自动求导（AUTOGRAD）
          4.1.4 神经网络包torch.nn
     4.2  线性回归、逻辑回归和深度神经网络的Pytorch实现
          4.2.1 线性回归的Pytorch实现
          4.2.2 逻辑回归的Pytorch实现
          4.2.3 深度神经网络的Pytorch实现
     4.3  TensorFlow基础知识
          4.3.1 安装
          4.3.2 张量（Tensors）
          4.3.3 自动求导（AUTOGRAD）
          4.3.4 神经网络包
     4.4  线性回归、逻辑回归和深度神经网络的TensorFlow实现
          4.4.1 线性回归的TensorFlow实现
          4.4.2 逻辑回归的TensorFlow实现
          4.4.3 深度神经网络的TensorFlow实现
        
第5章  卷积神经网络
       5.1 卷积
         1. 1  卷积和特征提取
         1.2   卷积的步长和填充
         1.3   多输入通道和多输出通道
       5.2  池化层
       5.3  卷积神经⽹络(CNN)
           5.3.1  LeNet网络
           5.3.2  参数初始化 
           5.3.3  识别MNIST手写数字
           5.3.4  CIFAR-10 图像分类     
       5.4  打开卷积神经网络的“黑盒子”
           5.4.1  卷积神经网络为什么有效
           5.4.2  卷积神经网络的可视化
           5.4.3  进一步理解卷积神经网络? 
             5.4.3.1 类显著度图
             5.4.3.2 欺骗神经网络（含对抗例子）
      5.5  卷积神经网络的典型架构
          5.5.1   AlexNet模型
          5.5.2   VGG模型
          5.5.3   NIN模型
          5.5.4   Google  Net模型
          5.5.5   残差网络ResNet
          5.5.6   稠密网络DenseNet

第6章  优化
      6.1 网络优化
         6.1.1 参数初始化
         6.1.2 激活函数
      6.2  regularization 技术          
          6.2.1  Dropout
          6.2.2  批归一化
          6.2.3  层归一化   
      6.3 优化算法
           6.3.1  动量法
           6.3.2  AdaGrad
           6.3.3  RMSProp
           6.3.4  AdaDelta
           6.3.5  Adam
      6.4  实战（待补充，可以产生对比效果）
      
第7章  循环神经网络
     7.1  文本和序列问题
     7.2  循环神经网络模型架构
     7.3  通过时间的反向传播     
     7.4  GRU和LSTM
     7.5  实战：情感分类
     7.6  注意力机制
     7.7 实战：实现机器翻译
        
第8章  生成对抗网络
      8.1  什么是生成对抗网络       
      8.2  GAN具体原理
      8.3  经典GAN结构     
      8.4  实战：生成动漫角色
      8.5  实战：实现图片域的迁移

第9章  深度学习在计算机视觉领域的应用
       9.1  目标检测
            9.1.1 什么是目标检测？
            9.1.2 经典目标检测算法介绍
            9.1.3 深度学习目标检测
            9.1.4 实战：车牌检测
       9.2 风格迁移
           9.2.1 风格迁移的原理
           9.2.2 实战：风格迁移的实现 
       9.3
           9.3.1 DeepDream的原理
           9.3.2 实战：DeepDream实现
       9.4 其他应用介绍
       
第10章  深度学习在自然语言处理领域的应用
       10.1 自然语言理解
          10.1.1 词向量
          10.1.2 传统的Word2Vec 和 FastTest
          10.1.3 新型的BERT
          10.1.4 实战： 近义词
          10.1.5 实战： 聊天机器人       
       10.2   ??? 
       10.3  其他应用介绍
  
第11章 强化学习
       11.1  什么是强化学习
       11.2 Q-Learning
       11.3  实战：Q-Learning训练flappy bird小游戏
       11.4  DQN
       11.5  实战：DQN训练flappy bird小游戏
       11.6  其他算法介绍
```

如强化学习：
```
基于神经网络的深度学习近年来在计算机视觉、机器翻译和时间序列预测等问题上取得许多突破，但它们也可以与强化学习算法相结合，创造出令人震惊的东西吗，如击败人类围棋冠军的AlphGo、在“蛋白质折叠” 的结构预测的竞赛CASP上碾压人类专家的AlphaFold（首次参赛的 AlphaFold 就在 98 位参赛者获得冠军，在对总共 43 种蛋白质的预测中，AlphaFold 获得了其中 25 种的最高分数，排名第二的队伍获得最高分数的蛋白质只有 3 种。）。

在监督式学习中，学习算法需要学习输入x和输出y之间的函数关系，当一个输入x输入到学习得到的函数模型中，可以得到一个输出y，为了学习这个函数关系，需要提供一组样本，对每个样本中的x都给出了明确的输出y，即每个样本都有一个“正确的答案或目标”。

但对许多序列决策或控制问题，如下棋算法需要在每一步给出最佳的走子决策，对于这类问题，算法并没有一个明确的答案y来指导如何学习，如下棋的每一步走子是否能导致最终的胜利并没有一个明确的答案。对这种一系列序列决策问题，虽然每一步的决策没有明确的答案用于指导学习，但每一步决策都可得到来自外部环境的反馈，即所谓的“奖励或惩罚（负的奖励）”，强化学习在这种来自外部环境的“奖励”的指导下，不断调整优化决策，学习如何在多个步骤中获得最大的“奖励或回报”。如下棋程序中就是通过每一步选择最佳走子获得最后的胜利。再如在游戏中，每一步选择一个最佳动作，通过一系列动作最大化游戏中赢得的积分。

正如一个小孩在成长过程中，因为做错事收到惩罚如走路摔倒就回疼痛，因为做得好而受到鼓励或奖励一样，通过每个动作或决定的不同而得到不同的奖励或惩罚，小孩会逐步学会正确的决策从而获得最大的奖励或回报。

因此，强化学习是一个解决序列决策或控制问题的面向目标的算法，在与环境的交互过程中，作为强化学习算法的智能体采取的每个动作都会得到来自外部环境的正或负的奖励，智能体会逐渐学习在每个步骤或时刻应该采取哪个动作以获得最大化的奖励。

强化学习和监督式学习、非监督式学习一起构成了机器学习的三种方法。

强化学习解决了将即时行动与其产生的延迟回报关联起来的难题。与人类一样，强化学习算法有时需要等待一段时间才能看到决策的成果。它们在延迟的反馈环境中运行，在这种环境中，很难理解在多个时间步骤的哪个行为导致了何种结果。

强化学习算法可以在更加模糊、真实的环境中表现得越来越好，同时从任意数量的可能动作中进行选择，而不是从视频游戏的有限选项中进行选择。

由于其一般性，强化学习在许多其他学科中进行了研究，例如博弈论，控制理论，运筹学，信息论，基于模拟的优化，多智能体系统，群体智能，统计学和遗传算法。

强化学习已经成功地应用于各种应用如计算机游戏、自主直升机飞行、机器人腿部运动、手机网络路由、营销策略选择、工厂控制和高效的网页索引。完胜AlphGo的AlphaZero不需要借助于人类围棋专家的大量对局案例，通过强化学习算法的自我对弈（即双手互博），训练34小时的AlphaZero就胜过了训练72小时的AlphaGo。 基于强化学习的游戏智能算法如DeepMind的星际争霸强化学习算法已经完胜人类游戏玩家。可以预期，强化学习将在也现实世界中得到越来越多的应用。

本章将从强化学习的理论基础“马可夫决策过程（Markov decision processes (MDP)）”开始，介绍强化学习的基本概念、原理和典型算法如时差算法、Q-Learning、A3C等，并给出这些算法的实现过程和案例应用。
```
