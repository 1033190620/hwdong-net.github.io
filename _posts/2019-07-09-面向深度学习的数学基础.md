
---
layout:       post
title:        面向深度学习的数学基础
subtitle:     Mathematical basics for deep learning 
date:         2019-07-09 19:59:00
author:       "xuepro"
header-img:   "img/home_bg.jpg"
header-mask:  0.3
catalog:      true
multilingual: true
tags:
    - DL    
---    

#### 极限

极限描述的是一种无限逼近，比如有一个无限的数列如${\{1,1/2，1/3，\cdots,1/n,\cdots\}}$。当n不断增大时，数列中对应的数如1/n将越来越小，越来越接近值0，也就是无限逼近0或者说这个数列随着n无限增大，将逐渐收敛到其0。这个0就称为这个数列的**极限**。所谓无限逼近，就是说，只要n充分大，1/n和其极限值0的距离就足够小。或者说对于一个任意小的数如$\epsilon=0.001 $，总能找到一个n，使得从这个第n项之后的数列中的所有数和极限0的距离都小于这个$\epsilon=0.001 $。例如从n=1000项后，$|1/n-0|<\epsilon$。再如数列${\{3-1,3-1/2，3-1/3，\cdots,3-1/n,\cdots\}}$的极限时3。列${\{0.3,0.33，0.333，0.333,\cdots\}}$的极限时10/3。用下面的记号$\lim $表示极限：

$$\lim \limits_{n \to \infty}{\{1,1/2，1/3，\cdots,1/n,\cdots\}} = 0 $$

同样，可以定义函数f(x)在某一点$x_0$处的极限$\lim \limits_{ x \to x_0} f(x)$。如：
$$\lim \limits_{ x \to 3} x^2 = 9$$

表示当自变量x逼近3，因变量f(x)值逼近9，即f(x)的极限是9。例如一组逼近3的自变量数列${\{3-1,3-1/2,3-1/3,\cdots, \}}$对应的f(x)序列是${\{(3-1)^2,(3-1/2)^2,(3-1/3)^2,\cdots, \}}$的极限是9。
#### 导数

设函数$y=f(x)$在点$x_0$的某个邻域内有定义，当自变量$x$在$x_0$处有增量$\Delta x$，$(x0+\Delta x)$也在该邻域内时，相应地函数取得增量$\Delta y=f(x_0+\Delta x)−f(x_0)$，如果$\Delta y$与$\Delta x$之比当$\Delta x \rightarrow 0$ 时极限存在，则称函数$y=f(x)$在点$x_0$处可导，并称这个极限为函数$y=f(x)$在点$x_0$处的导数，记作$f′(x_0)$，即：

$$f'(x_0)=\lim \limits_{\Delta x \to 0} \frac{\Delta y}{\Delta x}=\lim \limits_{\Delta x \to 0} \frac{f(x_0+\Delta x)-f(x_0)}{\Delta x}$$

如果函数$y=f(x)$在开区间内每一点都可导，就称函数$f(x)$在区间内可导。这时函数$y=f(x)$对于区间内的每一个确定的$x$值，都对应着一个确定的导数值，这就构成一个新的函数，称这个函数为原来函数$y=f(x)$的导函数，记作$f′(x)$。

不严谨地说，导数就是函数在每个点上因变量y关于自变量x的变化率（$\frac{\Delta y}{\Delta x}$的极限），当$\Delta x$很小时，导数$f'(x)$可以用$\frac{\Delta y}{\Delta x}$近似。
$$f'(x)\simeq \frac{\Delta y}{\Delta x} = \frac{f(x+\Delta x)-f(x)}{\Delta x}$$

或者说因变量f(x)的增量可近似表示为$f'(x)$和自变量增量$\Delta x$的乘积，即：

$$ f(x+\Delta x)-f(x) \simeq f'(x) \Delta x $$

如果将$(x,y)$看成二维平面上的一个坐标点，函数$y=f(x)$看成是二维平面上的曲线，则导数$f(x_0)$就是$f(x)$在点$x_0$处切线的斜率。如图所示。

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQi1pVCyzFJQ9pWvoAw8D0mtVzukcH1hLEVQYqirA6BqF3I9FzwiQ)


#### 导数的四则运算

当2个函数通过四则运算（加、减、乘、除）构造一个新的函数时，新的函数的导数可以用这2个函数的导数计算得到，其计算公式如下：
$$(f(x)+g(x))'=f'(x)+g'(x)$$

$$(f(x)-g(x))'=f'(x)-g'(x)$$

$$(f(x)g(x))'=f'(x)g(x)+f(x)g’(x)$$

$$(\frac{f(x)}{g(x)})'=\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}$$

证明过程就是根据导数的定义，感兴趣读者可以查阅微积分教材。

#### 复合函数求导的链式法则

设$f(x)$和$g(x)$是可导函数，$h(x) = f(g(x))$，则$h'(x)=f'(g(x))g'(x)$。

####  多变量函数的偏导数与梯度

函数$f: \mathbb{R}^n \rightarrow \mathbb{R}$，$f(x)$是一个多变量函数，关于每个$x=(x_1,x_1,\cdots,x_n)$分量$x_j$的导数称为偏导数，记为$\frac{\partial {f} }{\partial {x_j} }$，反应了$f(x)$关于这个分量$x_j$的变化率。

$$\frac{\partial {f} }{\partial {x_j} }=\lim \limits_{\Delta x_j \to 0} \frac{f(x_1,\cdots,x_j+\Delta {x_j},\cdots,x_n)-f(x_1,\cdots,x_j,\cdots,x_n)}{\Delta{x_j}}$$

$f(x)$关于$x$的梯度$\nabla f(x)$是$f(x)$关于x的每个分量$x_j$的偏导数构成的向量：

$$ \nabla f(x)=\frac{d \boldsymbol{f}}{d \mathbf{x}} = (\frac{\partial {f} }{\partial {x_1} },\cdots,\frac{\partial {f} }{\partial {x_j} },\cdots,\frac{\partial {f} }{\partial {x_n} })\in \mathbb{R}^{ n}$$

对于很小的$x$的增量$\Delta x = (\Delta x_1,\Delta x_2,\cdots,\Delta x_n)$，$f(x)$的增量$f(x+\Delta)-f(x)$可以近似表示为梯度$\nabla f(x)$和x增量$\Delta x$的点积。

$$f(x+\Delta)-f(x) \simeq \nabla f(x) \cdot \Delta x $$

向量通常由2种写法：行向量和列向量分别将一个向量写成行或列的形式。如果将$x$、$\Delta x$和梯度$\nabla f(x)$都写成列向量形式：
$$ x=\begin {bmatrix}
x_1\\
x_2\\
\vdots   \\
x_n\\
\end {bmatrix}
\quad
\Delta x=\begin {bmatrix}
\Delta x_1\\
\Delta x_2\\
\vdots   \\
\Delta x_n\\
\end {bmatrix}
 \quad
\nabla f(x)=\begin {bmatrix}
\frac{\partial {f} }{\partial {x_1} }\\
\frac{\partial {f} }{\partial {x_2} }\\
\vdots   \\
\frac{\partial {f} }{\partial {x_n} }\\
\end {bmatrix} 
$$

则梯度向量和增量向量的点积可以写成矩阵乘积的形式：

$$f(x+\Delta)-f(x) \simeq {\nabla f(x)}^T  \Delta x =  {\Delta x}^T \nabla f(x) $$


如果梯度写成行向量形式：
$$\nabla_x f(x) = (\frac{\partial {f} }{\partial {x_1} },\cdots,\frac{\partial {f} }{\partial {x_j} },\cdots,\frac{\partial {f} }{\partial {x_n} })\in \mathbb{R}^{ n}$$

则梯度向量和增量向量的点积可以写成矩阵乘积的形式：
$$f(x+\Delta)-f(x) \simeq \nabla f(x)  \Delta x $$


对于一个多变量函数y=f(x)，如果自变量x写成矩阵的形式，则y关于x的梯度，虽然是一个向量，但有时也习惯写成一个矩阵形式：
$$ f'(x) = \frac {d y}{d \bf x} =\begin {bmatrix} \frac{\partial y }{\partial x_{11} } & \frac{\partial   y }{\partial x_{21}  }& \cdots & \frac{\partial y  }{\partial x_{n1} }  \\  \frac{\partial  y}{\partial x_{12}  } & \frac{\partial y}{\partial x_{22} }   & \cdots &  \frac{\partial y}{\partial x_{n2} }  \\ \vdots & \vdots & \ddots & \vdots \\  \frac{\partial y }{\partial x_{1n}  } &  \frac{\partial y }{\partial x_{2n} } & \cdots &  \frac{\partial y}{\partial x_{nn} }  \end{bmatrix}\tag{8}$$

将这个梯度写成行向量、列向量还是矩阵形式，完全取决于哪种形式更有助于你推导相关公式。如果x写成矩阵形式，梯度写成矩阵形式，看起来比较一致。

**例：** 设$y = w_1*x_1+ w_2*x_2+\dots+ w_n*x_n+b$，如果将y看成$w = (w_1,w_2,\dots,w_n)$的函数，则$\frac{d y}{d w } = (x_1,x_2,\dots,x_n)$；如果将y看成$x = (x_1,x_2,\dots,x_n)$的函数，则$\frac{d y}{d x } =  (w_1,w_2,\dots,w_n)$；如果将y看成$b$的函数，则$\frac{d y}{d b} =  1$。

####  向量值函数的导数与$\text{Jacobian}$矩阵


函数$\boldsymbol{f} : \mathbb{R} \rightarrow \mathbb{R}^m$，即f是由多个单变量函数$f_i(x)$构成的一个向量值函数，每个函数$f_i(x)$都有一个关于x的导数，这些导数堆积成一个向量，称为向量值函数关于自变量x的导数：

$$
 f'(x)=\frac{d \boldsymbol{f}}{d \mathbf{x}} = 
\begin {bmatrix}
\frac{d f_1}{d x} \\
\frac{d f_2}{d x}\\
\vdots   \\
\frac{d f_m}{d x} 
\end {bmatrix}
\in \mathbb{R}^{m \times 1}
$$


函数$\boldsymbol{f} : \mathbb{R}^n \rightarrow \mathbb{R}^m$，即f是由多个多变量函数$f_i(x)$构成的一个向量值函数，每个函数$f_i(x)$都有一个关于x的梯度，这些梯度向量堆积在一个，得到一个矩阵，称为$\text{Jacobian}$矩阵为：

$$
f'(x)=\frac{d \boldsymbol{f}}{d \mathbf{x}} = 
\begin {bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots &  \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_n} \\
\vdots & \vdots & \ddots \\
\frac{\partial f_m}{\partial x_1} &\frac{\partial f_m}{\partial x_2} & \cdots & \frac{\partial f_m}{\partial x_n}
\end {bmatrix}
\in \mathbb{R}^{m \times n}
$$

**注意**：如果将一个函数的梯度写成列向量形式，则$\text{Jacobian}$矩阵就是上述矩阵的转置矩阵。

一个向量$x=(x_1,x_2,\cdots,x_n)$可以看成它自身的一个多变量向量值函数，其导数就是一个恒等矩阵$I$:

$$\frac{d x}{dx}= \begin {bmatrix}
1 & 0 & \cdots &  0 \\
0 & 1 & \cdots &  0 \\
\vdots & \vdots & \ddots \\
0 & 0 & \cdots &  1 \\
\end {bmatrix} = I $$

**例：** 设$\begin{align} y = \begin{bmatrix} y_{1}\\
y_{2}  \\
\vdots  \\ 
y_{m} \\
\end{bmatrix}= \begin{bmatrix} w_{11} & w_{12} & \dots & w_{1n} \\
w_{21} & w_{22} & \dots & w_{2n} \\
\vdots & \vdots & \ddots & \vdots \\ 
w_{m1} & w_{m2} & \dots & w_{mn} \\
\end{bmatrix}\cdot \begin{bmatrix} x_{1}\\
x_{2}  \\
\vdots  \\ 
x_{n} \\
\end{bmatrix} +\begin{bmatrix} b_{1}\\
b_{2}  \\
\vdots  \\ 
b_{m} \\
\end{bmatrix} =\begin{bmatrix} w_{11}*x_1+ w_{12}*x_2*+\dots+ w_{1n}*x_n+b_{1}\\
w_{21}*x_1+ w_{22}*x_2*+\dots+ w_{2n}*x_n+b_{2}  \\
\vdots  \\ 
w_{m1}*x_1+ w_{m2}*x_2*+\dots+ w_{mn}*x_n+b_{m} \\
\end{bmatrix}\end{align}$
，如果将y看成$x = (x_1,x_2,\cdots,x_n)$的多变量向量值函数，则$\frac{d y}{d x }$的$\text{Jacobian}$矩阵为：

$$
f'(x)=\frac{d \boldsymbol{y}}{d \mathbf{x}} = 
\begin {bmatrix}
w_{11} & w_{11}  & \cdots &  w_{1n} \\
w_{21} & w_{21}  & \cdots &  w_{2n} \\\\
\vdots & \vdots & \ddots \\
w_{m1} & w_{m1}  & \cdots &  w_{mn} \\
\end {bmatrix}
\in \mathbb{R}^{m \times n}
$$

；如果将y看成$x = (b_1,b_2,\cdots,b_m)$的多变量向量值函数，则$\frac{d y}{d b }$的$\text{Jacobian}$矩阵为：

$$
f'(b)=\frac{d \boldsymbol{y}}{d \mathbf{b}} = 
\begin {bmatrix}
1 & 0  & \cdots &  0 \\
0 & 1  & \cdots &  0 \\\\
\vdots & \vdots & \ddots \\
0 & 0  & \cdots & 1 \\
\end {bmatrix}
\in \mathbb{R}^{m \times m}
$$

；如果将y看成$w = (w_{11},w_{12},\cdots,w_{1n},\cdots,w_{m1},w_{m2},\cdots,w_{mn})$的多变量向量值函数，则$\frac{d y}{d w }$的$\text{Jacobian}$矩阵为：
$$
f'(w)=\frac{d \boldsymbol{y}}{d \mathbf{w}} = 
\begin {bmatrix}
x_1 & \cdots & x_n     & 0 &\cdots &0    & \cdots\cdots  & 0 &\cdots &0\\
0 & \cdots & 0     & x_1 &\cdots &x_n    & \cdots\cdots  & 0 &\cdots &0\\
\vdots & \vdots &\ddots  &\vdots & \vdots &\ddots  &\vdots & \vdots &\ddots      \\
0 & \cdots & 0     & 0 &\cdots &0    & \cdots\cdots  & x_1 &\cdots &x_n\\
\end {bmatrix}
\in \mathbb{R}^{m \times (m\times n)}
$$

因为每个$y_i$依赖的$w_i = (w_{i1},w_{i1},\cdots,w_{in})$相互之间是不相干的，可以将$y_i$看成只依赖$w_i = (w_{i1},w_{i1},\cdots,w_{in})$的n个自变量的函数。那么$y$关于$w$的导数或$\text{Jacobian}$矩阵也可写成下面更简洁的形式：

$$
f'(w)=\frac{d \boldsymbol{y}}{d \mathbf{w}} = 
\begin {bmatrix}
x_1 & x_2 & \cdots & x_n  \\
x_1 & x_2 & \cdots & x_n  \\
\vdots & \vdots &\ddots  \\
x_1 & x_2 & \cdots & x_n  \\
\end {bmatrix}
\in \mathbb{R}^{m \times n}
$$

因为每一行都是相同的向量，甚至可以直接将f'(w)写成行向量或列向量的形式，如写成行向量形式：
$$f'(w)=\frac{d \boldsymbol{y}}{d \mathbf{w}} = (x_1 , x_2 , \cdots, x_n)$$

或列向量形式：
$$f'(w) = \frac{d \boldsymbol{y}}{d \mathbf{w}}= \begin {bmatrix}
x_1\\
x_2\\
\vdots  \\
x_n\\
\end {bmatrix} $$



**例：** 设$ z(x) = \begin{bmatrix} z_{1}(x)\\
z_{2}(x)  \\
\end{bmatrix}= \begin{bmatrix} 2 x_1 + 4 x_2 +7x_3 \\
3 x_1 +5 x_2 +  4 x_3 \\
\end{bmatrix}$是x的函数，而$y =  \begin{bmatrix} 4 z_1 + 3 z_2  \\
\end{bmatrix}$是z的函数，则 f(x) = y(z(x))是y(z)和z(x)的复合函数，根据复合函数求导规则，有：

$$f'(x) = y'(z)z'(x) = (4,3)  \begin{bmatrix} 2 & 4& 7 \\
3 &5 & 4  \\
\end{bmatrix} = (17,31,40)$$

展开f(x)的完整表达式$f(x) = 4(2 x_1 + 4 x_2 +7x_3)+3(3 x_1 +5 x_2 +  4 x_3)  =17x_1 + 31x_2 + 40x_3$，可以验证上述的结果是正确的。

如果约定将梯度写成列向量形式，则链式法则就要倒过来写，

$$y'(z) = \begin{bmatrix} 4\\3 \\
\end{bmatrix} \quad
z'(x) = \begin{bmatrix} 2 & 3\\
4 &5  \\
7 &4  \\
\end{bmatrix}  \quad
f'(x) = z'(x)y'(z) = \begin{bmatrix} 2 & 3\\
4 &5  \\
7 &4  \\
\end{bmatrix}\begin{bmatrix} 4\\3 \\
\end{bmatrix}= \begin{bmatrix} 17\\
31\\
40\\
\end{bmatrix}$$

在今后推导这些公式时，一定要注意梯度等向量是列向量还是行向量。深度学习的反向传播求导过程经常采用的是列向量的形式，如Pytorch的backward()自动求导函数就是这样计算梯度的。

**例：** 设$ W= \begin{bmatrix} w_{11} & w_{12} & \dots & w_{1n} \\
w_{21} & w_{22} & \dots & w_{2n} \\
\vdots & \vdots & \ddots & \vdots \\ 
w_{m1} & w_{m2} & \dots & w_{mn} \\
\end{bmatrix} \quad
 x = \begin{bmatrix} x_{1}\\
x_{2}  \\
\vdots  \\ 
x_{n} \\
\end{bmatrix}\quad
b = \begin{bmatrix} b_{1}\\
b_{2}  \\
\vdots  \\ 
b_{m} \\
\end{bmatrix}\quad
\hat{z} = \begin{bmatrix} \hat{z}_{1}\\
\hat{z}_{2}  \\
\vdots  \\ 
\hat{z}_{m} \\
\end{bmatrix}\quad
$， $z= Wx+b$，即：
$$
\begin{align} z = \begin{bmatrix} z_{1}\\
z_{2}  \\
\vdots  \\ 
z_{m} \\
\end{bmatrix}= \begin{bmatrix} w_{11} & w_{12} & \dots & w_{1n} \\
w_{21} & w_{22} & \dots & w_{2n} \\
\vdots & \vdots & \ddots & \vdots \\ 
w_{m1} & w_{m2} & \dots & w_{mn} \\
\end{bmatrix}\cdot \begin{bmatrix} x_{1}\\
x_{2}  \\
\vdots  \\ 
x_{n} \\
\end{bmatrix} +\begin{bmatrix} b_{1}\\
b_{2}  \\
\vdots  \\ 
b_{m} \\
\end{bmatrix} =\begin{bmatrix} w_{11}*x_1+ w_{12}*x_2*+\dots+ w_{1n}*x_n+b_{1}\\
w_{21}*x_1+ w_{22}*x_2*+\dots+ w_{2n}*x_n+b_{2}  \\
\vdots  \\ 
w_{m1}*x_1+ w_{m2}*x_2*+\dots+ w_{mn}*x_n+b_{m} \\
\end{bmatrix}\end{align}$$
，而$y = \frac{1}{2} {(z-\hat{z})}^2 $。如何计算y关于W和x的导数？

**解**：假设梯度等都是行向量形式，则：
   $$y'(z) = (z_1,z_2,\cdots,z_m) = z^T  $$
   $$z'(x) = W  $$
   
   $$z'(W) = \begin {bmatrix}
x_1 & \cdots & x_n     & 0 &\cdots &0    & \cdots\cdots  & 0 &\cdots &0\\
0 & \cdots & 0     & x_1 &\cdots &x_n    & \cdots\cdots  & 0 &\cdots &0\\
\vdots & \vdots &\ddots  &\vdots & \vdots &\ddots  &\vdots & \vdots &\ddots      \\
0 & \cdots & 0     & 0 &\cdots &0    & \cdots\cdots  & x_1 &\cdots &x_n\\
\end {bmatrix}$$
 
$$ \begin{align}
  y'(x) &= y'(z)z'(x) = z^T W  =[z_1,z_2,\cdots,z_m] \cdot \begin{bmatrix} w_{11} & w_{12} & \dots & w_{1n} \\
w_{21} & w_{22} & \dots & w_{2n} \\
\vdots & \vdots & \ddots & \vdots \\ 
w_{m1} & w_{m2} & \dots & w_{mn} \\
\end{bmatrix} \\ &= [z_1*w_{11}+ z_2*w_{21}+\cdots+ z_m*w_{m1}, \quad \cdots,\quad z_1*w_{1n}+ z_2*w_{2n}+\cdots+ z_m*w_{mn}]\end{align}$$

$$ \begin{align}
  y'(W) &= y'(z)z'(W)  =[z_1,z_2,\cdots,z_m] \cdot \begin {bmatrix}
x_1 & \cdots & x_n     & 0 &\cdots &0    & \cdots\cdots  & 0 &\cdots &0\\
0 & \cdots & 0     & x_1 &\cdots &x_n    & \cdots\cdots  & 0 &\cdots &0\\
\vdots & \vdots &\ddots  &\vdots & \vdots &\ddots  &\vdots & \vdots &\ddots      \\
0 & \cdots & 0     & 0 &\cdots &0    & \cdots\cdots  & x_1 &\cdots &x_n\\
\end {bmatrix} \\ &= [z_1*x_1,z_1*x_2,\cdots,z_1*x_n, \quad \cdots \quad, z_m*x_1,z_m*x_2,\cdots,z_m*x_n ]\end{align}$$

当然为了观看美观，可以将$y'(W)$写成和W一样的矩阵形式：

$$y'(W) = \begin{bmatrix} z_1*x_1  &z_1*x_2& \dots & z_1*x_n \\
z_2*x_1  &z_2*x_2& \dots & z_2*x_n \\
\vdots & \vdots & \ddots & \vdots \\ 
z_m*x_1  &z_m*x_2& \dots & z_m*x_n \\
\end{bmatrix}= z x^T = x z^T$$

尽管数学上，$z'(W)$是一个$m\times (m\times n)$矩阵，$y'(W)$是一个$m\times n$向量，为了将$y'(W) $写成矩阵形式，可以令
$z'(W) = x$的，则：$$y'(W) = z'(W)y'(z) = x z^T$$

**注意**：这仅仅是一种书写方式或记忆法，而不是验证的数学。


假设梯度等都是列向量形式：
   $$y'(z) = z  $$
   $$z'(x) = W^T  $$
并将$z'(W)$简记为如下的形式：
   $$z'(W) = x^T  $$
$$ \begin{align}
  y'(x) = z'(x)y'(z) =  W^T z
\end{align}$$

$$ \begin{align}
  y'(W) &= y'(z) z'(W) =   zx^T 
\end{align}$$

#### 矩阵值函数的导数

假如由一个矩阵
$\begin{align} Y = \begin{bmatrix} Y_{11} & Y_{12} & \dots & Y_{1n} \\
Y_{21} & Y_{22} & \dots & Y_{2n} \\
\vdots & \vdots & \ddots & \vdots \\ 
Y_{m1} & Y_{m2} & \dots & Y_{mn} \\
\end{bmatrix}\end{align}$，其元素都是某个标量x的函数，即这个矩阵是由$m\times n$个单变量函数$Y_{ij}(x)$构成的矩阵函数。其关于变量x的导数为：

$\begin{align} \frac{d \mathbf{Y}}{d \mathbf{x}} = \begin{bmatrix} \frac{d Y_{11}}{d x} & \frac{d Y_{12}}{d x} & \dots & \frac{d Y_{1n}}{d x} \\ 
\frac{d Y_{21}}{d x} & \frac{d Y_{22}}{d x} & \dots & \frac{d Y_{2n}}{d x} \\ 
 \vdots & \vdots & \ddots & \vdots \\  
\frac{d Y_{m1}}{d x} & \frac{d Y_{m2}}{d x} & \dots & \frac{d Y_{mn}}{d x} \\ 
\end{bmatrix}\end{align}$
