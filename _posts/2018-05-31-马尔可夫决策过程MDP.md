---
layout:       post
title:        "马尔可夫决策过程MDP"
subtitle:     "MDP"
date:         2018-05-28 19:12:00
author:       "xuepro"
header-img:   "img/home_bg.jpg"
header-mask:  0.3
catalog:      true
multilingual: true
tags:
    - AI
    
---

**A stochastic process** is an indexed collection of random variables $$ {X_t} $$.
 e.g., time series of weekly demands for a product
 
**A stochastic process $$ X_t $$** is said to be Markovian if and only if

$$ P(X_{t+1}=j| X_t=i,X_{t-1}=k_{t-1},X_{t-2}=k_{t-2},...X_1=k_1,X_0=k_0) = P(X_{t+1}=j|X_0=k_0) $$

   “The future is independent of the past given the present”
   
**A Markov process** (or **Markov chain**) is a **memoryless** stochastic process, i.e., a sequence of random states
$$s_1; s_2; : : :$$ with the Markov property

**A Markov process**  is a is a tuple $$ S;P; \mu $$

* S is a (finite) set of states
* P is a state transition probability matrix, $$P_{ss\'}$$ = $$P(s\'|s) $$
 $$P_{ss\'} = P(s\'|s) $$
 
* a set of initial probabilities $$ \mu_j^0 = P(X_0=i)$$ for all i

**A Markov reward process** is a Markov process with values.
A Markov Reward Process is a tuple $$S;P;R;\gamma; \mu $$

* S is a (finite) set of states
* P is a state transition probability matrix, $$P_{ss\'} = P(s\'|s) $$

* **R** is a reward function,$$ R_s = E[r&#124s] $$
* **$$\gamma$$ is a discount factor,  $ \gamm \in[0,0]$
* a set of initial probabilities $$ \mu_j^0 = P(X_0=i)$$ for all i


refer:

[List of Greek letters and math symbols](https://www.sharelatex.com/learn/List_of_Greek_letters_and_math_symbols)

[2](http://web.ift.uib.no/Teori/KURS/WRK/TeX/symALL.html)
